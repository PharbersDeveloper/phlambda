version: 0.0.0
framework: pyspark
engine: spark
language: python
language_version: 3.8.9
code:
  phmain:
    args:
      - key: inputs
        index: 0
      - key: output
        index: 1
      - key: project_id
        index: 2
      - key: project_name
        index: 3
      - key: runtime
        index: 4
      - key: dag_name
        index: 5
      - key: script_name
        index: 6
    code: |
      import os
      import re
      import json
      import time
      import boto3
      import click
      import traceback

      from phjob import execute
      from pyspark.sql import SparkSession
      from pyspark.sql.functions import lit, col, struct, to_json, json_tuple, when, split
      from pyspark.sql.types import StructType, StringType, DoubleType, LongType
      from phcli.ph_logs.ph_logs import phs3logger, LOG_DEBUG_LEVEL
      from functools import reduce
      from clickhouse_driver import Client
      from boto3.dynamodb.conditions import Key

      tenant = "pharbers"
      lake_prefix = f"s3://ph-platform/2020-11-11/lake/{tenant}/"
      dynamodb_resource = boto3.resource("dynamodb", region_name="cn-northwest-1")

      # 获取Spark实例
      os.environ["PYSPARK_PYTHON"] = "python3"
      spark = SparkSession.builder.master("yarn")
      default_config = {
          "spark.sql.codegen.wholeStage": False,
          "spark.sql.execution.arrow.pyspark.enabled": "true",
      }
      for k, v in default_config.items():
          spark = spark.config(k, v)
      spark = spark.enableHiveSupport().getOrCreate()
      access_key = os.getenv("AWS_ACCESS_KEY_ID")
      secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")
      if access_key is not None:
          spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", access_key)
          spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", secret_key)
          spark._jsc.hadoopConfiguration().set("com.amazonaws.services.s3.enableV4", "true")
          spark._jsc.hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
          spark._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "s3.cn-northwest-1.amazonaws.com.cn")


      # 解决在读取中间型DF Schema Type冲突后的选择
      # 在同Schema Col类型冲突后优先选择非String类型的描述 list(filter(lambda item: item["type"] != "String", values))
      # 这部分在后续扩展时不是很友好，原因是后续会扩展很多类型
      def conflict_schema_type_resolve(df):
          from itertools import groupby
          rows = df.select("traceId", "schema").distinct().withColumn("__run_id", split("traceId", "\+00:00")[0]).orderBy(
              "__run_id", ascending=True).collect()
          schema_strs = list(reduce(lambda schema_pre, schema_next: set(schema_pre).union(set(schema_next)),
                                    list(map(lambda row: [f"{schema['name']}:{schema['type']}" for schema in
                                                          json.loads(row["schema"])], rows))))
          group_schemas = groupby(
              list(map(lambda item: {"src": item.split(":")[0], "type": item.split(":")[1].capitalize()}, schema_strs)),
              key=lambda x: x["src"])
          schemas = []
          for key, group in group_schemas:
              values = list(group)
              if len(values) > 1:
                  values = list(filter(lambda item: item["type"] != "String", values))
              schemas += values
          return schemas


      # 根据Schema类型转换DF类型
      def transform_schema_type(df, schemas):
          pattern = "^-?[0-9]*.?[0-9]*$rep"

          choose_schema_type = {
              "string": StringType(),
              "double": DoubleType(),
              "long": LongType(),
              "bigint": LongType()
          }

          def transform_number(default="double"):
              transform_df = df.withColumn(f"""__{item["src"]}_type""",
                                           when(col(item["src"]).rlike(pattern), "true").when(col(item["src"]).isNull(),
                                                                                              "true").otherwise("false"))
              if transform_df.filter(f"""`__{item["src"]}_type` = 'false'""").count() == 0:
                  transform_df = transform_df.withColumn(item["src"], col(item["src"]).cast(choose_schema_type[default]))
              else:
                  transform_df = transform_df.withColumn(item["src"], col(item["src"]).cast(StringType()))
              return transform_df

          def transform_string(default="string"):
              return df.withColumn(item["src"], col(item["src"]).cast(choose_schema_type[default]))

          choose_schema_func = {
              "string": transform_string,
              "double": transform_number,
              "long": transform_number
          }

          types = list(set(map(lambda item: item["type"].lower(), schemas)))
          if len(types) == 1 and types[0] == "string":
              return df

          for item in schemas:
              df = choose_schema_func[item["type"].lower()](item["type"].lower()).drop(f"""__{item["src"]}_type""")
          return df


      # 获取DataFrame Schema转JSON字符串
      def get_df_schema2json(df):
          schema_list = list(filter(lambda item: item["name"] != "traceId",
                                    list(map(lambda item: {"name": item["name"], "type": item["type"]},
                                             df.schema.jsonValue()["fields"]))))
          return json.dumps(schema_list, ensure_ascii=False)


      # 将DataFrame统一Schema一致性
      def convert_consistency_schema(df, schema):
          return df.withColumn("data",
                               to_json(struct(*[col(c) for c in list(map(lambda item: item["name"], json.loads(schema)))]))) \
              .withColumn("schema", lit(schema)).select("traceId", "schema", "data")


      # 对多TraceId取Schema的col的并集（暂时先不管类型）****************
      def convert_union_schema(df):
          rows = df.select("traceId", "schema").distinct().collect()
          return list(reduce(lambda schema_pre, schema_next: set(schema_pre).union(set(schema_next)),
                             list(map(lambda row: [schema["name"] for schema in json.loads(row["schema"])], rows))))


      # 将统一Schema的DF转成正常的DataFrame
      def convert_normal_df(df, cols):
          return df.select(col("traceId"), json_tuple(col("data"), *cols)).toDF("traceId", *cols)


      # 从spark df转成pandas df
      def transform_spark_df2_other_df(runtime, df_map):
          transform_df_map = {}
          for key, df in df_map.items():
              transform_df_map.update({key: df.toPandas() if runtime == "python3" else df})
          return transform_df_map


      # 从pandas转成spark df
      def transform_other_df2_spark_df(runtime, df):
          if runtime == "python3":
              return spark.createDataFrame(df)
          return df


      # 获取DynamoDB数据
      def get_dynamodb_item(table_name, key):
          table = dynamodb_resource.Table(table_name)
          res = table.get_item(Key=key)
          return res.get("Item")


      # DynamoDB增加更新操作
      def put_dynamodb_item(table_name, item):
          table = dynamodb_resource.Table(table_name)
          table.put_item(Item=item)


      # Dynamodb 添加version
      def put_item_to_version(output_id, project_id, output_version, owner):
          table_name = "version"
          version_item = {
              "id": f"{project_id}_{output_id}",
              "projectId": project_id,
              "datasetId": output_id,
              "name": output_version,
              "date": str(int(round(time.time() * 1000))),
              "owner": owner
          }
          put_dynamodb_item(table_name, version_item)


      # dataset 更新Schema
      def put_scheme_to_dataset(output_id, project_id, df, output_version):
          schema_list = list(
              map(lambda item: {"src": item[0], "des": item[0], "type": item[1].capitalize().replace("Int", "Double")},
                  df.dtypes))
          key = {"id": output_id, "projectId": project_id}
          dataset_item = get_dynamodb_item("dataset", key)
          dataset_item.update({"schema": json.dumps(schema_list, ensure_ascii=False)})
          dataset_item.update({"version": json.dumps(output_version, ensure_ascii=False)})
          put_dynamodb_item("dataset", dataset_item)


      # 根据DynamoDB index查询
      def get_ds_with_index(ds_name, project_id):
          ds_table = dynamodb_resource.Table("dataset")
          return ds_table.query(
              IndexName="dataset-projectId-name-index",
              KeyConditionExpression=Key("projectId").eq(project_id) & Key("name").begins_with(ds_name)
          )["Items"][0]


      # 添加Column 如:version
      def add_column(df, value, col_name="version"):
          return df.withColumn(col_name, lit(value))


      # 读取S3文件转为DF
      def s3_to_df(suffix, path, versions, options=None):
          def reader_csv():
              if options:
                  return spark.read.schema(options).csv(path)
              return spark.read.csv(path, header=True)

          def reader_parquet():
              return spark.read.parquet(path)

          reader_funcs = {
              "csv": reader_csv,
              "parquet": reader_parquet
          }
          df = reader_funcs[suffix]()
          if len(versions) != 0:
              df = df.where(df["traceId"].isin(versions))
          return df


      # 读取Glue数据转为DF
      def catalog_to_df(database, table):
          return spark.sql(f"SELECT * FROM {database}.{table}")


      # 根据cat类型、path来生成DF实体
      def generate_df(input_ds, output_version, project_id, is_single_run, logger):
          cat = input_ds.get("cat")
          versions = input_ds.get("version")
          if cat != "uploaded" and cat != "input_index" and cat != "catalog":
              cat = "parquet"
              if len(versions) == 0 and not is_single_run:
                  versions.append(output_version)

          name = input_ds.get("name")
          path = f"{lake_prefix}{project_id}/{name}"
          suffix = path.split(".")[-1].lower()
          if suffix != "csv":
              suffix = "parquet"

          schemas = json.loads(get_ds_with_index(name, project_id).get("schema"))

          def load_s3_df():
              return s3_to_df(suffix, path, versions)

          def uploaded_s3_df():
              schema_type = StructType()
              for item in schemas:
                  schema_type = schema_type.add(item["src"], StringType())
              return transform_schema_type(s3_to_df("csv", path, versions, schema_type), schemas)

          def load_catalog_df():
              prop = input_ds.get("prop")
              database = prop.get("databaseName")
              table = prop.get("tableName")
              return transform_schema_type(catalog_to_df(database, table), schemas)

          def load_input_index_df():
              prop = input_ds.get("prop")
              return transform_schema_type(s3_to_df("csv", prop.get("path"), [], []), schemas)

          cat_funcs = {
              "uploaded": uploaded_s3_df,
              "catalog": load_catalog_df,
              "input_index": load_input_index_df,
              "parquet": load_s3_df
          }

          df = cat_funcs[cat]()
          if cat == "parquet":
              parquet_df = convert_normal_df(df, convert_union_schema(df)).drop("traceId")
              df = transform_schema_type(parquet_df, conflict_schema_type_resolve(df))
          return df.na.fill("None")


      # build DF参数
      def build_inputs_df(runtime, inputs, kwargs, project_id, output_version, logger):
          df_map = {}
          ds_list = kwargs.get("ds_conf")
          name_ds_map_dict = {}
          for ds in ds_list:
              name_ds_map_dict.update({ds.get("name"): ds})
          for input_ds_name in inputs:
              if input_ds_name in name_ds_map_dict.keys():
                  input_df = generate_df(name_ds_map_dict.get(input_ds_name), output_version, project_id, True, logger)
                  df_map.update({f"df_{input_ds_name}": input_df})
              else:
                  input_ds = {"name": input_ds_name, "version": output_version.split(","), "cat": "intermediate"}
                  input_df = generate_df(input_ds, output_version, project_id, False, logger)
                  df_map.update({f"df_{input_ds_name}": input_df})
          return transform_spark_df2_other_df(runtime, df_map)


      # build Create Sql
      def build_create_sql(df, table_name, database="default", order_by="", partition_by="version"):
          sql_cols = list(map(lambda item: f"`{item[0]}` {item[1].capitalize()}" if item[0] in [order_by, partition_by]
          else f"`{item[0]}` Nullable({item[1].capitalize()})", df.dtypes))
          sql_schema = ",".join(sql_cols)
          return f"CREATE TABLE IF NOT EXISTS {database}.`{table_name}`({sql_schema}) ENGINE = MergeTree() ORDER BY tuple({order_by}) PARTITION BY {partition_by};"


      # build Sample DF
      def build_sample_df(output_id, project_id, df):
          table_name = "dataset"
          key = {
              "id": output_id,
              "projectId": project_id
          }
          dataset_item = get_dynamodb_item(table_name, key)
          sample = dataset_item.get("sample", "F_1").split("_")
          sample_rule = sample[0]
          df_count = sample[1]

          def sample_first():
              return df.limit(int(df_count) * 10000)

          def sample_random():
              fraction = float(int(df_count) * 20000 / df.count())
              if fraction >= 1.0:
                  fraction = 1.0
              return df.sample(withReplacement=False, fraction=fraction).limit(int(df_count) * 10000)

          sample_strategy = {
              "F": sample_first,
              "R": sample_random
          }
          return sample_strategy[sample_rule]()


      # 写最终输出的数据
      def write_output_df(ph_conf, output_version, ch_client,
                          df, tenant_ip, project_id, table, output_id,
                          mode="append", number_partitions=2, database="default"):
          table_name = "_".join(table.split("_")[1:])
          path = f"{lake_prefix}{project_id}/{table_name}/"

          # 将数据转为多级Schema存入S3
          convert_consistency_schema(df, get_df_schema2json(df)) \
              .repartition(number_partitions).write.format("parquet") \
              .mode(mode) \
              .partitionBy("traceId") \
              .save(path)

          # 先判断 clickhouse 中是否有数据
          sql = f"SELECT COUNT(*) from `{table}`"
          res = ch_client.execute(sql)
          count = res[0][0]
          if count == 0:
              # 进行sample 策略
              sample_df = build_sample_df(output_id, project_id, df).drop("traceId")

              # 写入clickhouse
              sample_df.write.format("jdbc").mode(mode) \
                  .option("url", f"jdbc:clickhouse://{tenant_ip}:8123/{database}") \
                  .option("dbtable", f"`{table}`") \
                  .option("driver", "ru.yandex.clickhouse.ClickHouseDriver") \
                  .option("user", "default") \
                  .option("password", "") \
                  .option("batchsize", 1000) \
                  .option("socket_timeout", 300000) \
                  .option("numPartitions", number_partitions) \
                  .option("rewrtieBatchedStatements", True) \
                  .save()

              # 更新Schema
              put_scheme_to_dataset(output_id, project_id, sample_df, output_version)

          # 插入新version
          put_item_to_version(output_id, project_id, output_version, ph_conf.get("ownerId"))


      # build output DF
      def build_output_df(runtime, kwargs, ph_conf, output_ds_name, output_version, tenant_ip, project_id):
          out_df = transform_other_df2_spark_df(runtime, kwargs.get("out_df"))
          if out_df:
              table_name = project_id + "_" + output_ds_name
              output_id = get_ds_with_index(output_ds_name, project_id).get("id")
              ch_client = Client(host=tenant_ip)
              out_df = add_column(out_df, output_version, col_name="version")

              # 保持原有产品层级的version，新增traceId内容与version一致
              add_traceId_df = add_column(out_df, output_version, col_name='traceId')
              sql_create_table = build_create_sql(out_df, table_name)

              # 因为schema可能会与原不一致导致插入错误，在此之前会进行删除表操作
              ch_client.execute(f"DROP TABLE IF EXISTS default.`{project_id}_{output_ds_name}`")

              ch_client.execute(sql_create_table)

              write_output_df(ph_conf, output_version, ch_client, add_traceId_df, tenant_ip, project_id, table_name,
                              output_id)


      @click.command()
      @click.option("--owner")
      @click.option("--dag_name")
      @click.option("--run_id")
      @click.option("--job_full_name")
      @click.option("--project_ip")
      @click.option("--ph_conf")
      def debug_execute(**kwargs):
          logger = phs3logger("emr_log", LOG_DEBUG_LEVEL)
          try:
              dag_name = "$dag_name"
              script_name = "$script_name"
              args = {"name": f"{dag_name}_{script_name}"}
              inputs = $inputs
              output = "$output"
              project_id = "$project_id"
              project_name = "$project_name"
              runtime = "$runtime"
              ip = kwargs.get("project_ip")
              ph_conf = json.loads(kwargs.get("ph_conf", {}))
              user_conf = ph_conf.get("userConf", {})
              ds_conf = ph_conf.get("datasets", {})
              args.update(user_conf)
              args.update({"ds_conf": ds_conf})
              args.update({"spark": spark})
              args.update(kwargs)
              output_version = args.get("run_id") + "_" + ph_conf.get("showName")

              df_map = build_inputs_df(runtime, inputs, args, project_id, output_version, logger)

              args.update(df_map)
              result = execute(**args)
              args.update(result if isinstance(result, dict) else {})

              build_output_df(runtime, args, ph_conf, output, output_version, ip, project_id)

              return result
          except Exception as e:
              logger.error(traceback.format_exc())
              print(traceback.format_exc())
              raise e


      if __name__ == '__main__':
          debug_execute()




  phjob:
    input_var_args: ~
    code: |
      def execute(**kwargs):
          data_frame = None
          $input_var_args

          return {"out_df": data_frame}

