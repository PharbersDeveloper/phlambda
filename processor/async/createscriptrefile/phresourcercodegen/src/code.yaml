version: 0.0.0
framework: sparkr
engine: spark
language: r
language_version: 3.6.3
code:
  phmain:
    args:
      - key: inputs
        index: 0
      - key: output
        index: 1
      - key: project_id
        index: 2
      - key: project_name
        index: 3
      - key: runtime
        index: 4
      - key: dag_name
        index: 5
      - key: script_name
        index: 6
    code: |
      Sys.setlocale(category="LC_ALL",locale="zh_CN.UTF-8")
      library(SparkR)
      library(jsonlite)
      library(httr)
      library(paws)
      library(stringr)

      cmd_args <- commandArgs(T)

      owner <- cmd_args[1]
      dag_name <- cmd_args[2]
      run_id <- cmd_args[3]
      job_full_name <- cmd_args[4]
      job_id <- cmd_args[5]
      project_ip <- cmd_args[6]
      ph_conf <- cmd_args[7]
      tenant <- "pharbers"

      Encoding(owner)
      Encoding(owner) <- "UTF-8"
      Encoding(ph_conf)
      Encoding(ph_conf) <- "UTF-8"
      Encoding(dag_name)
      Encoding(dag_name) <- "UTF-8"
      Encoding(job_full_name)
      Encoding(job_full_name) <- "UTF-8"

      input_args <- list(
          owner = owner, dag_name = dag_name, run_id = run_id,
          job_full_name = job_full_name, job_id = job_id, ph_conf = ph_conf,
          project_ip = project_ip
      )

      sparkJars <- "s3://ph-platform/2020-11-11/emr/client/clickhouse-connector/clickhouse-jdbc-0.2.4.jar,s3://ph-platform/2020-11-11/emr/client/clickhouse-connector/guava-30.1.1-jre.jar"

      click_house_jdbc_url <- paste("jdbc:clickhouse://", project_ip, ":8123", sep = "")

      click_house_jdbc_driver <- "ru.yandex.clickhouse.ClickHouseDriver"

      lake_prefix <- paste("s3://ph-platform/2020-11-11/lake/", tenant, "/", sep = "")


      spark <- sparkR.session(
          sparkConfig = list(
              sparkJars = sparkJars,
              spark.sql.execution.arrow.sparkr.enabled = "true")
      )

      print("Spark Init")


      db <- dynamodb(
          config = list(
              region = "cn-northwest-1"
          )
      )

      # 解决在读取中间型DF Schema Type冲突后的选择
      # 在同Schema Col类型冲突后优先选择非String类型的描述 list(filter(lambda item: item["type"] != "String", values))
      # 这部分在后续扩展时不是很友好，原因是后续会扩展很多类型
      conflict_schema_type_resolve <- function(df) {
          select_df <- SparkR::selectExpr(df, "traceId", "schema", "split(traceId, '\\\\+00:00')[0] as __run_id")
          rows <- SparkR::collect(SparkR::distinct(select_df))
          cols <- list()
          union_cols <- list()
          structured_union_cols = list()
          for (row in split(rows, 1:nrow(rows))) {
              content <- list()
              for (item in fromJSON(row$schema, simplifyVector = FALSE)) {
                  content <- append(content, paste(item$name, ":", item$type, sep=""))
              }
              cols <- append(cols, list(content))
          }

          for (item in cols) { union_cols <- union(union_cols, item) }

          for (item in union_cols) {
              src <- str_split(item, ":", simplify=TRUE)[1]
              type <- str_split(item, ":", simplify=TRUE)[2]
              structured_union_cols <- append(structured_union_cols, list(list(src=src, type=type)))
          }

          r_df <- as.data.frame(structured_union_cols[1])
          for (item in structured_union_cols[2:length(structured_union_cols)]) {
              r_df <- rbind(r_df, as.data.frame(item))
          }

          r_df <- SparkR::collect(SparkR::withColumnRenamed(SparkR::agg(SparkR::group_by(SparkR::createDataFrame(r_df), "src"),  type = "collect_list"), "collect_list(type)", "type"))


          schemas <- list()
          total_schemas <- list()
          for (item in split(r_df, 1:nrow(r_df))) {total_schemas <- append(total_schemas, list(as.list(item)))}


          for (item in total_schemas) {
              type <- item$type[[1]][[1]]
              if (length(item$type[[1]]) > 1) {
                  type <- item$type[[1]][!(item$type[[1]] %in% "string")][[1]]
              }
              schemas <- append(schemas, list(list(src=item[[1]][1], type=str_to_title(type))))
          }
          return(schemas)
      }


      # 根据Schema类型转换DF类型
      transform_schema_type <- function(df, schemas) {
          pattern <- "^-?[0-9]*.?[0-9]*$"
          choose_schema_type <- list(
              string="string",
              double="double",
              float="double",
              long="long",
              bigint="long"
          )

          transform_number <- function(col_name, default="double") {
              append_col_name = paste("__", col_name, "_type", sep = "")
              transform_df <- SparkR::withColumn(df, append_col_name, SparkR::otherwise(SparkR::when(SparkR::rlike(df[[col_name]], pattern) | SparkR::isNull(df[[col_name]]), "true"), "false"))
              if (SparkR::count(SparkR::filter(transform_df, paste("`", append_col_name , "` ", "=", "false", sep = ""))) == 0) {
                  transform_df <- SparkR::withColumn(transform_df, col_name, SparkR::cast(df[[col_name]], choose_schema_type[[default]]))
              } else {
                  transform_df <- SparkR::withColumn(transform_df, col_name, SparkR::cast(df[[col_name]], "string"))
              }
              return(transform_df)
          }

          transform_string <- function(col_name, default="string") {
              return(SparkR::withColumn(df, col_name, SparkR::cast(df[[col_name]], choose_schema_type[[default]])))
          }

          choose_schema_function <- list(
              string = transform_string,
              double = transform_number,
              float = transform_number,
              long = transform_number,
              bigint = transform_number
          )

          types <- list()

          for (item in schemas) {
              types <- append(tolower(item$type), types)
          }

          types <- unique(types)

          if (length(types) == 1 && types[1] == "string") {
              return(df)
          }

          for (item in schemas) {
              type_name <- tolower(item$type)
              df <- SparkR::drop(choose_schema_function[[type_name]](item$src, type_name), paste("__", item$src, "_type", sep = ""))
          }
          return(df)
      }

      # 获取df schema 转成 json 字符串
      get_schema2str <- function(df) {
          schemas <- list()
          for (item in SparkR::dtypes(df)) {
              schema <- strsplit(item, '" "')
              if (schema[[1]] != "traceId") {
                  content <- list(name=schema[[1]], type=schema[[2]])
                  schemas <- append(schemas, list(content))
              }
          }
          return(paste0(toJSON(schemas, auto_unbox = TRUE), sep=""))
      }

      # 将DF统一Schema一致性
      convert_consistency_schema <- function(df, schema) {
          cols <- list()
          for (item in fromJSON(schema, simplifyVector = FALSE)) {
              content <- paste("`", item$name ,"`", sep="")
              cols <- append(cols, content)
          }
          combine_df <- SparkR::selectExpr(df, "*", paste("to_json(struct(", paste(cols, collapse = ','), ")) as data", sep=""))
          return(SparkR::select(SparkR::withColumn(combine_df, "schema", schema), "traceId", "schema", "data"))
      }

      # 对多TraceId取Schema的col的并集
      convert_union_schema <- function(df) {
          rows <- SparkR::collect(SparkR::distinct(select(df, "traceId", "schema")))
          cols <- list()
          union_cols <- list()
          for (row in split(rows, 1:nrow(rows))) {
              content <- list()
              for (item in fromJSON(row$schema, simplifyVector = FALSE)) {
                  content <- append(content, item$name)
              }
              cols <- append(cols, list(content))
          }

          for (item in cols) {
              union_cols <- union(union_cols, item)
          }
          return(union_cols)
      }

      # 将统一Schema的DF转成正常的 Data Frame
      convert_normal_df <- function(df, schema_cols) {
          cols <- list()
          for (item in schema_cols) {
              content <- paste("`", item, "` STRING", sep="")
              cols <- append(cols, content)
          }
          result_df <- SparkR::withColumn(df, "json", from_json(df$data, paste(cols, collapse = ",")))
          return(SparkR::select(result_df, "traceId", "json.*"))
      }


      create_table_http_call <- function(sql) {
          url <- paste("http://", project_ip,
                      ":8123", "/?query=", URLencode(sql),
                      sep = ""
          )
          return(gsub("[\n]", "", POST(url)))
      }

      get_ds_item <- function(project_id, name) {
          result <- db$query(
              TableName = "dataset",
              IndexName = "dataset-projectId-name-index",
              ExpressionAttributeNames = list(
                  "#name" = "name"
              ),
              ExpressionAttributeValues = list(
                  `:projectId` = list( S = project_id ),
                  `:dsName` = list( S = name )
              ),
              KeyConditionExpression = "projectId = :projectId AND #name = :dsName",
          )$Item

          result_item <- list()

          for(item in result) {
              for (key in names(item)) {
                  value <- item[[key]]$S
                  if (length(value) == 0) {
                      value <- item[[key]]$N
                  }
                  if (value == "") {
                      value <- " "
                  }
                  Encoding(value)
                  Encoding(value) <- "UTF-8"

                  if (length(item[[key]]$S) == 0) {
                      result_item[[key]] <- list(N=value)
                  } else {
                      result_item[[key]] <- list(S=value)
                  }
              }
          }
          return(result_item)
      }

      put_dy_item <- function(table, item) {
          db$put_item(
              Item = item,
              TableName = table,
          )
      }

      handle_msg_to_ds <- function(project_id, name) {
          ds_name <- paste(project_id, "_", name, sep = "")
          schema <- list()
          item <- get_ds_item(project_id, name)
          result <- create_table_http_call(paste("SELECT name, type from system.columns where database='default' AND table='", ds_name, "' FORMAT JSON", sep = ""))

          for (data in fromJSON(result, simplifyVector = FALSE)$data) {
              content <- list(src = data$name, des = data$name, type = data$type)
              schema <- append(schema, list(content))
          }
          schema_str <- paste0(toJSON(schema, auto_unbox = TRUE), spe = "")
          item$schema <- list(S = schema_str)
          put_dy_item("dataset", item)
      }

      handle_msg_to_version <- function(project_id, output_id, output_version, owner) {
          item = list(
              id = list(
                  S = paste(project_id, "_", output_id, sep = "")
              ),
              projectId = list(
                  S = project_id
              ),
              datasetId = list(
                  S = output_id
              ),
              name = list(
                  S = output_version
              ),
              date = list(
                  S = paste(round(as.numeric(Sys.time()), 0) * 1000, sep = "")
              ),
              owner = list(
                  S = owner
              )
          )
          put_dy_item("version", item)
      }

      create_table_sql <- function(df, table_name, database="default") {
          get_schema <- function(df) {
              column_types <- SparkR::dtypes(df)
              sql_schema <- ""
              for (ct in column_types) {
                  item <- strsplit(ct, '" "')
                  coltype <- str_to_title(item[[2]])
                  col <- item[[1]]
                  col_str <- ""
                  if (col != "version") {
                      col_str <- paste("`", col, "` ", "Nullable(",coltype, ")", sep = "")
                  } else {
                      col_str <- paste("`", col, "` ", coltype, sep = "")
                  }
                  sql_schema <- paste(sql_schema, col_str, ",", sep = "")
              }
              sql_schema <- sub(",$", "", sql_schema)
              return(sql_schema)
          }
          return(paste("CREATE TABLE IF NOT EXISTS ", database, ".",
                      "`", table_name, "`", "(", get_schema(df), ") ",
                      "ENGINE = MergeTree() PRIMARY KEY version ORDER BY version;",
                      sep = ""
          ))
      }

      s3_to_df <- function(path, version) {
          suffix_str <- strsplit(path, split = ".", fixed = TRUE)
          num_suffix <- length(suffix_str[[1]])
          suffix <- tolower(suffix_str[[1]][num_suffix])

          read_csv <- function() {
              return(SparkR::read.df(path, "csv", header = TRUE))
          }

          read_parquet <- function() {
              return(SparkR::read.df(path))
          }

          read_s3_funcs <- list(csv = read_csv, parquet = read_parquet)

          if (suffix != "csv") {
              suffix <- "parquet"
          }

          df <- read_s3_funcs[[suffix]]()

          if (length(version) != 0) {
              df <- SparkR::filter(df, df$traceId %in% version)
          }
          return(df)
      }

      catalog_to_df <- function(database, table) {
          return(SparkR::sql(paste("SELECT * FROM ", database, ".", table, sep = "")))
      }

      load_df <- function(input_ds, output_version, project_id, project_name, is_single_run) {

          cat <- input_ds$cat
          version <- input_ds$version
          if (cat != "uploaded" && cat != "input_index" && cat != "catalog") {
              cat <- "parquet"
              if (length(version) == 0 & !is_single_run) {
                  version <- append(version, output_version)
              }
          }
          name <- input_ds$name
          schema_types <- list()
          convert_schemas <- list()

          result <- get_ds_item(project_id, name)
          for (item in fromJSON(result$schema$S, simplifyVector = FALSE)) {
              content <- paste("`", item$src, "` STRING", sep="")
              schema_types <- append(schema_types, content)
          }

          for (item in fromJSON(result$schema$S, simplifyVector = FALSE)) {
              content <- list(
                  src=item$src,
                  des=item$des,
                  type=item$type
              )
              convert_schemas <- append(convert_schemas, list(content))
          }

          load_s3_df <- function() {
              path <- paste(lake_prefix, project_id, "/", name, sep = "")
              return(s3_to_df(path, version))
          }

          uploaded_s3_df <- function() {
              df <- transform_schema_type(
                  SparkR::read.df(paste(lake_prefix, project_id, "/", name, sep = ""), "csv", paste(schema_types, collapse = ",")),
                  convert_schemas
              )
              if (length(version) != 0) {
                  df <- SparkR::filter(df, df$version %in% version)
              }
              return(df)
          }

          load_catalog_df <- function() {
              prop <- input_ds$prop
              database <- prop$databaseName
              table <- prop$tableName
              return(transform_schema_type(catalog_to_df(database, table), convert_schemas))
          }

          load_input_index_df <- function() {
              prop <- input_ds$prop
              path <- prop$path
              return(transform_schema_type(s3_to_df(path, list()), convert_schemas))
          }

          cat_funcs <- list(
              uploaded = uploaded_s3_df,
              catalog = load_catalog_df,
              input_index = load_input_index_df,
              parquet = load_s3_df
          )

          df <- cat_funcs[[cat]]()
          if (cat == "parquet") {
              parquet_df <- SparkR::drop(convert_normal_df(df, convert_union_schema(df)), "traceId")
              df <- transform_schema_type(parquet_df, conflict_schema_type_resolve(df))
              return(SparkR::fillna(df, "None"))
          }
          return(SparkR::fillna(df, "None"))
      }

      # spark DF转换成R DF
      transform_spark_df2_other_df <- function(runtime, df_map) {
          keys <- names(df_map)
          transform_df_map <- list()
          for (key in keys) {
              if (runtime == "sparkr") {
                  transform_df_map[[key]] <- df_map[[key]]
              } else {
                  transform_df_map[[key]] <- SparkR::collect(df_map[[key]])
              }
          }
          return(transform_df_map)
      }

      # R DF转换成spark DF
      transform_other_df2_spark_df <- function(runtime, df) {
          if (runtime == "sparkr") {
              return(df)
          } else {
              return(SparkR::createDataFrame(df))
          }
      }

      read_df <- function(inputs, kwargs, project_id, project_name, output_version) {
          df_map <- list()
          ds_list <- kwargs$ds_conf
          name_ds_map_dict <- list()

          for (ds in ds_list) {
              key <- ds$name
              name_ds_map_dict[[key]] <- ds
          }

          for (input_ds_name in inputs) {
              Encoding(input_ds_name)
              Encoding(input_ds_name) <- "UTF-8"
              key <- paste("df_", input_ds_name, sep = "")
              if (input_ds_name %in% names(name_ds_map_dict)) {
                  input_df <- load_df(
                      name_ds_map_dict[[input_ds_name]],
                      output_version, project_id, project_name, TRUE
                  )
                  df_map[[key]] <- input_df
              } else {
                  input_ds <- list(
                      name = input_ds_name,
                      version = strsplit(output_version, split = ","),
                      cat = "intermediate"
                  )
                  input_df <- load_df(
                      input_ds, output_version,
                      project_id, project_name, FALSE
                  )
                  df_map[[key]] <- input_df
              }
          }
          return(df_map)
      }

      create_input_df <- function(
              runtime, inputs,
              args, project_id,
              project_name, output_version) {
          Encoding(output_version)
          Encoding(output_version) <- "UTF-8"
          df_map <- read_df(inputs, args, project_id, project_name, output_version)
          return(transform_spark_df2_other_df(runtime, df_map))
      }

      out_to_s3 <- function(df, ds_conf) {
          prop <- ds_conf$prop
          ds_format <- tolower(prop$format)
          path <- prop$path
          partitions <- prop$partitions

          consistency_df <- convert_consistency_schema(df, get_schema2str(df))

          to_parquet <- function() {
              SparkR::write.df(
                  repartition(consistency_df, partitions), path,
                  "parquet", "append",
                  partitionBy = "traceId"
              )
          }

          to_csv <- function() {
              SparkR::write.df(
                  repartition(consistency_df, partitions), path,
                  "csv", "overwrite",
                  header = TRUE
              )
          }

          write_funcs <- list(parquet = to_parquet, csv = to_csv)

          write_funcs[[ds_format]]()
      }

      out_click_house <- function(df, table) {
          user <- "default"
          password <- ""
          SparkR::write.jdbc(limit(df, 10000), click_house_jdbc_url, table, "append",
                  user = user,
                  password = password, driver = click_house_jdbc_driver,
                  batchsize = "10000", queryTimeout = "300000"
          )
      }

      add_column <- function(df, version, version_colname="version") {
          return(SparkR::withColumn(df, version_colname, version))
      }

      create_outputs <- function(runtime, args, ph_conf, output, project_id, project_name, output_version) {

          Encoding(output_version)
          Encoding(output_version) <- "UTF-8"
          out_df <- transform_other_df2_spark_df(runtime, args$out_df)

          table_name <- paste(project_id, "_", output, sep = "")
          Encoding(table_name)
          Encoding(table_name) <- "UTF-8"

          normal_out_df <- add_column(out_df, output_version, "version")
          normal_out_df <- SparkR::drop(normal_out_df, "traceId")

          ds_list <- args$ds_conf

          name_ds_map_dict <- list()

          for (ds in ds_list) {
              name_ds_map_dict[[ds$name]] <- ds
          }

          # 因为schema可能会与原不一致导致插入错误，在此之前会进行删除表操作
          create_table_http_call(paste("DROP TABLE IF EXISTS default.","`", table_name, "`", sep = ""))

          create_table_http_call(create_table_sql(normal_out_df, table_name))

          count_sql <- paste("SELECT COUNT(*) FROM ", "`", table_name, "`", sep = "")
          response <- create_table_http_call(count_sql)
          path <- paste(lake_prefix, project_id, "/", output, "/", sep = "")
          Encoding(path)
          Encoding(path) <- "UTF-8"

          if (response == "0") {
              out_click_house(normal_out_df, paste("`", table_name, "`", sep = ""))
          }

          # 保持原有产品层级的version，新增traceId内容与version一致
          add_traceId_df <- add_column(normal_out_df, output_version, "traceId")

          out_to_s3(
              add_traceId_df,
              list(prop = list(format = "Parquet", path = path, partitions = 2))
          )

          output_result <- get_ds_item(project_id, output)
          output_id <- output_result$id$S

          handle_msg_to_ds(project_id, output)

          handle_msg_to_version(project_id, output_id, output_version, owner)
      }

      source("phjob.R")

      dag_name <- "$$dag_name"
      script_name <- "$$script_name"

      args <- list(name=paste(dag_name, "_", script_name, sep=""))

      inputs <- list($$inputs)

      output <- "$$output"

      project_id <- "$$project_id"
      project_name <- "$$project_name"
      runtime <- "$$runtime"

      ph_conf <- fromJSON(input_args$ph_conf, simplifyVector=FALSE)

      user_conf <- ph_conf$userConf

      ds_conf <- as.list(ph_conf$datasets)

      args <- c(args, user_conf)
      args[["ds_conf"]] <- ds_conf
      args <- c(args, input_args)

      output_version <- paste(args$run_id, "_", ph_conf$showName, sep="")

      df_map <- create_input_df(runtime, inputs, args, project_id, project_name, output_version)

      args <- c(args, df_map)

      result <- exec(args)

      args <- c(args, result)

      create_outputs(runtime, args, ph_conf, output, project_id, project_name, output_version)

  phjob:
    input_var_args: ~
    code: |
      exec <- function(cmd_args) {
          $$input_var_args

          return(list(out_df=data_frame))
      }


