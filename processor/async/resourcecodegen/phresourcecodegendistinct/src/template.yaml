template:
  phjob.py:
    args:
      - key: a
        Index: 0
    content: |-
        from pyspark.sql.functions import col
        from pyspark.sql.types import StructType, StringType
        from pyspark.sql import functions as func
        from pyspark.sql import Window
           
    
        def execute(**kwargs):
            spark = kwargs['spark']
            df = kwargs['df_$g_input$']
            
            g_preFilter = $g_preFilter$
            g_postFilter = $g_postFilter$
            g_distinct_key = $g_distinct_key$
            g_globalCount = $g_globalCount$
            
            # args_preFilter 处理
            if g_preFilter['enabled'] == True:
                df = df.where(g_preFilter['expression'])
            
            # globalCount 计算
            if g_globalCount == True:
                cols_globalCount = (g_distinct_key if len(g_distinct_key) > 0 else df.columns)
                df = df.withColumn('count', func.count(cols_globalCount[0]).over(Window.partitionBy(cols_globalCount).orderBy()) )
            
            # distinct 处理
            if len(g_distinct_key) > 0:
                if g_globalCount == True:
                    df = df.select(g_distinct_key + ['count']).distinct()
                else:
                    df = df.select(g_distinct_key).distinct()
            else:
                df = df.distinct()
            
            # args_postFilter 处理
            if g_postFilter['enabled'] == True:
                df = df.where(g_postFilter['expression'])

            return {"out_df":df}