version: 0.0.0
framework: pyspark
engine: spark
language: python
language_version: 3.8.9
code:
  phjob:
    inputs: ~
    join_args: ~
    code: |
      from pyspark.sql.functions import *
      from pyspark.sql import *
      df_list = []

      def get_df(ds_name):
          return list(filter(lambda item: item["name"] == ds_name, df_list))[0]

      def pre_filter(args):
          ds_name = args["ds"]
          df_map = get_df(ds_name)
          df = df_map["df"]
          preFilter = args["preFilter"]
          enabled = preFilter["enabled"]
          distinct = preFilter["distinct"]
          expression = preFilter["expr"]
          if enabled:
              if distinct:
                  df = df.distinct()
              if expression != "":
                  df = df.filter(expression)
          df_map["df"] = df

      def rename_df_cols(args):
          ds_name = args["ds"]
          df_map = get_df(ds_name)
          df = df_map["df"]
          df_map["df"] = df.selectExpr(list(map(lambda item: item["value"], args["columns"])))

      def pre_join_computed_columns(args):
          if len(args) > 0:
              ds_name = args["ds"]
              df_map = get_df(ds_name)
              df = df_map["df"]
              computedColumns = args["computedColumns"]
              for item in computedColumns:
                  df = df.selectExpr('*', f"{item['expr']} AS `{item['name']}`").withColumn(item["name"], col(item["name"]).cast(item["type"]))
              df_map["df"] = df

      def select_columns(args):
          ds_name = args["ds"]
          prefix = args["prefix"]
          df_map = get_df(ds_name)
          df_cols = list(map(lambda item: item[0], df_map["df"].dtypes))

          def prefix_append(cols, falg):
              return list(map(lambda item: {"key": f"`{item}`", "value": f"`{item}` AS `{prefix}_{item}`", "selected": falg} if len(prefix) > 0 and prefix != " " else {"key": f"`{item}`", "value": f"`{item}`", "selected": falg}, cols))

          if args["type"] == "select":
              columns = args["columns"]
              diff_cols = prefix_append(list(set(df_cols) - set(columns)), False)
              select_cols = prefix_append(columns, True)
              all_cols = select_cols + diff_cols
          else:
              all_cols = prefix_append(df_cols, True)

          return {
              "ds": ds_name,
              "columns": all_cols
          }

      def post_join_computed_columns(df, args):
          if len(args) > 0:
              for item in args:
                  df = df.selectExpr('*', f"{item['expr']} AS `{item['name']}`").withColumn(item["name"], col(item["name"]).cast(item["type"]))
          return df

      def post_filter(df, args):
          distinct = args["distinct"]
          enabled = args["enabled"]
          expression = args["expr"]
          if distinct:
              df = df.distinct()
          if enabled and len(expression) > 0:
              df = df.filter(expression)
          return df

      def joins(select_cols ,args):
          on_func = {
              "=": "=="
          }
          def col_rename_handle(ds_name):
              return list(map(lambda col: col["value"], list(filter(lambda item: item["ds"] == ds_name, select_cols))[0]["columns"]))

          def col_on_handle(ds_name, col):
              columns = list(filter(lambda item: item["ds"] == ds_name, select_cols))[0]["columns"]
              return list(filter(lambda item: item["key"] == f"`{col}`", columns))[0]["value"].split(" ")[-1]

          def concat_select_df(ds_name):
              return f"""get_df("{ds_name}")["df"]"""

          def concat_on_cond(on_cond):
              def concat_exec(item):
                  return f"""get_df("{item["ds"]}")["df"]["{col_on_handle(item["ds"], item["column"])}"]"""
              return f" {on_func[on_cond['type']]} ".join(list(map(concat_exec, on_cond["conditions"])))

          ds_is_used = set()

          if args:
              first_join = args.pop(0) # args[0]
              join_str = f"""{concat_select_df(first_join["datasets"][0]["name"])}.join({concat_select_df(first_join["datasets"][1]["name"])}, [{",".join(list(map(concat_on_cond, first_join["on"])))}], "{first_join["type"]}")"""
              ds_is_used.add(first_join["datasets"][0]["name"])
              ds_is_used.add(first_join["datasets"][1]["name"])
              for item in args:
                  ds_name = list(set(map(lambda ds: ds["name"], item["datasets"])) - ds_is_used)
                  for ds in ds_name:
                      join_str += f""".join({concat_select_df(ds)}, [{",".join(list(map(concat_on_cond, item["on"])))}], "{item["type"]}")"""
              return eval(join_str)
          else:
              return None


      def execute(**kwargs):
          inputs = $inputs

          for ds_name in inputs:
              df_list.append({"name": ds_name, "df": kwargs["df_" + ds_name]})

          join_args = $join_args

          params = join_args["steps"][0]["expressions"]["params"]
          preFilters = params["preFilters"]
          preJoinComputedColumns = params["preJoinComputedColumns"]
          joinArgs = params["joins"]
          selectedColumns = params["selectedColumns"]
          postJoinComputedColumns = params["postJoinComputedColumns"]
          postFilter = params["postFilter"]

          for item in preFilters:
              pre_filter(item)

          for item in preJoinComputedColumns:
              pre_join_computed_columns(item)

          select_column_results = list(map(select_columns, selectedColumns))

          for item in select_column_results:
              rename_df_cols(item)

          df = joins(select_column_results, joinArgs)

          df = post_filter(post_join_computed_columns(df, postJoinComputedColumns), postFilter)

          selected_cols = []
          for item in select_column_results:
              selected_cols += list(map(lambda val: val["value"].split(" ")[-1], list(filter(lambda col: col["selected"] == True, item["columns"]))))

          df = df.selectExpr(selected_cols)

          return {"out_df": df}

