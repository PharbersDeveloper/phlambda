template:
  phjob.py:
    args:
      - key: a
        Index: 0
    content: |-
        from pyspark.sql.functions import *
        from pyspark.sql import functions as func
        from pyspark.sql import Window        
           
    
        def execute(**kwargs):
            spark = kwargs['spark']
            df = kwargs['df_$input$']
            
            args_preFilter = $args_preFilter$
            args_orders = $args_orders$
            args_denseRank = $args_denseRank$
            args_rank = $args_rank$
            args_rowNumber = $args_rowNumber$
            args_computedColumns = $args_computedColumns$

            # preFilter
            if args_preFilter['enabled'] == True:
                df = df.where(args_preFilter['expression'])
                if args_preFilter['distinct'] == True:
                    df = df.distinct()

            # order
            orders_col = list(map(lambda i:i['column'] , args_orders))
            orders_mode = list(map(lambda i:i['desc'] , args_orders))
            orders_col_mode = list(map(lambda i, y: col(i).desc() if y == True else col(i).asc(),  orders_col, orders_mode))

            df = df.orderBy( orders_col_mode )

            if args_denseRank == True:
                df = df.withColumn('denseRank',func.dense_rank().over(Window.partitionBy().orderBy( orders_col_mode )))

            if args_rank == True:
                df = df.withColumn('rank',func.rank().over(Window.partitionBy().orderBy( orders_col_mode )))

            if args_rowNumber == True:
                df = df.withColumn('rowNumber',func.row_number().over(Window.partitionBy().orderBy( orders_col_mode )))

            # computedColumns
            if len(args_computedColumns) > 0:
                for i_args in args_computedColumns:
                    df = df.withColumn(i_args['name'],  func.expr( i_args['expr'] ) ) \
                            .withColumn(i_args['name'], col(i_args['name']).cast( i_args['type'] ) )

            return {"out_df":df}