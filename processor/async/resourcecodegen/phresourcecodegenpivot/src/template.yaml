template:
  phjob.py:
    args:
      - key: a
        Index: 0
    content: |-
        import json
        import boto3
        from pyspark.sql.functions import *
        from pyspark.sql import functions as func
        from pyspark.sql import Window
        from functools import reduce
        from string import Template
        from boto3.dynamodb.conditions import Key
        from phcli.ph_logs.ph_logs import phs3logger, LOG_DEBUG_LEVEL
        import time


        dynamodb_resource = boto3.resource("dynamodb", region_name="cn-northwest-1")
        logger = phs3logger("emr_log", LOG_DEBUG_LEVEL)


        def get_ds_with_index(project_id, job_id):
            table = dynamodb_resource.Table("dagconf")
            return table.query(
                IndexName="dagconf-projectId-id-indexd",
                KeyConditionExpression=Key("projectId").eq(project_id) & Key("id").eq(job_id)
            )["Items"][0]


        def transform_type(tp, val):
            data_val_type = {
                "string": lambda x: str(x),
                "double": lambda x: float(x),
                "bigint": lambda x: int(x)
            }
            return data_val_type[tp](val)


        def code_free(args, prop, user_conf):
            args_str = json.dumps(args, ensure_ascii=False)

            def generate_data(item):
                name = f"${item['name']}"
                if name in user_conf:
                    return {name: transform_type(item["type"].lower(), user_conf[name])}
                else:
                    return {name: transform_type(item["type"].lower(), item["default"])}

            result_data = reduce(lambda p, n: {**p, **n}, list(map(generate_data, prop)))
            return json.loads(Template(args_str).substitute(
                reduce(lambda p, n: {**p, **n},
                       list(map(lambda key: {key[1:]: result_data[key]}, result_data.keys())))))


        def strAggFunc(g_l_keys, g_values):
            str_agg_single_func = []
            for i_g_values in g_values:
                if "customName" in i_g_values.keys():
                    g_customName = i_g_values['customName']
                    g_customExpr = i_g_values['customExpr']
                    g_type = i_g_values['type']
                    str_i_g_values = [f"{g_customExpr}.alias({g_customName}).cast({g_type})"]
                    str_agg_single_func.extend(str_i_g_values)
                else:
                    g_column = i_g_values['column']
                    # 简单函数处理 :"countDistinct", "min", "avg", "max", "count", "sum", "stddev"
                    single_func = list(i for i in ["countDistinct", "min", "avg", "max", "count", "sum", "stddev"]  if i_g_values[i] == True)
                    if len(single_func) > 0:
                        str_i_g_values = list(map(lambda i:f"{i}('{g_column}').alias('{g_column}_{i}')" , single_func))
                        str_agg_single_func.extend(str_i_g_values)

                    # first 和 last ( 不排序)
                    first_last_func = list(i for i in ["first", "last"]  if i_g_values[i] == True)
                    if len(first_last_func) > 0 and i_g_values['orderColumn'] =="":  
                        str_i_g_values = list(map(lambda i:f"{i}('{g_column}', ignorenulls={i_g_values['firstLastNotNull']}).alias('{g_column}_{i}')" , first_last_func))
                        str_agg_single_func.extend(str_i_g_values)

                    # concat
                    if i_g_values['concat'] == True:
                        str_collect = f"collect_set('{g_column}')" if i_g_values['concatDistinct'] == True else f"collect_list('{g_column}')"
                        str_i_g_values = [f" concat_ws('{i_g_values['concatSeparator']}', {str_collect} ).alias('{g_column}_concat') "]
                        str_agg_single_func.extend(str_i_g_values)            
            # 返回list：["sum('predict_sales').alias('predict_sales_sum')", "sum('predict_unit').alias('predict_unit_sum')"]
            print("strAggFunc:", str_agg_single_func)
            return str_agg_single_func


        def outFileToS3(df, time_flag, file_name):
            df.write.format("parquet").mode("append").save(f"s3://ph-platform/2020-11-11/etl/temporary_files/codefree_groupby_temp_out/{time_flag}/{file_name}")

        def readFileS3(time_flag, file_name, spark):
            df = spark.read.parquet(f"s3://ph-platform/2020-11-11/etl/temporary_files/codefree_groupby_temp_out/{time_flag}/{file_name}")
            return df

        def getOrderByLastFirst(df, g_l_keys, g_column, g_orderColumn, g_firstLastNotNull, g_last=False, g_first=False):
            if g_last == True:
                str_orderby = f"col('{g_orderColumn}').desc_nulls_last()" if g_firstLastNotNull == True else f"col('{g_orderColumn}').desc()"
                colname_type="last"
            if g_first == True:
                str_orderby = f"col('{g_orderColumn}').asc_nulls_last()" if g_firstLastNotNull == True else f"col('{g_orderColumn}').asc()"
                colname_type="first"
            # 用默认的升序排列：first为最小值-升序第一个， last则为最大值-降序第一个
            df_out = df.withColumn("row_number", func.row_number().over(Window.partitionBy(g_l_keys).orderBy( eval(str_orderby) ))) \
                        .where(col('row_number') == 1) \
                        .select(*g_l_keys, g_column) \
                        .withColumnRenamed(g_column, f"{g_column}_{colname_type}")
            return df_out

        def dealOrderByLastFirst(df, g_l_keys, g_values, time_flag):
            list_df_n = []
            for i, i_g_values in enumerate(g_values):
                g_column = i_g_values['column']
                g_orderColumn = i_g_values['orderColumn']
                g_first = i_g_values['first']
                g_last = i_g_values['last']
                g_firstLastNotNull = i_g_values['firstLastNotNull']

                # last, first:
                if g_orderColumn != "":
                    if g_last == True:
                        file_name = 'df_last_'+str(g_column)
                        df_last = getOrderByLastFirst(df, g_l_keys, g_column, g_orderColumn, g_firstLastNotNull, g_last=True, g_first=False)
                        list_df_n.append(file_name)
                        outFileToS3(df_last, time_flag, file_name)

                    if g_first == True:
                        file_name = 'df_first_'+str(g_column)
                        df_frist = getOrderByLastFirst(df, g_l_keys, g_column, g_orderColumn, g_firstLastNotNull, g_last=False, g_first=True)
                        list_df_n.append(file_name)
                        outFileToS3(df_frist, time_flag, file_name)

            return list_df_n

        def strJoinGroupDf(dict_df_all, dict_name, g_l_keys):
            str_join = ""
            for i, key in enumerate( dict_df_all.keys() ):
                if i == 0:
                    str_join = f"{dict_name}['{key}']"
                else:
                    str_join +=  f".join({dict_name}['{key}'], on={g_l_keys}, how='left')"
            return str_join

        def groupbyFunc(df, g_l_keys, g_values):
            # groupby
            list_df_all = []

            # 简单函数
            file_name = "SingleFunc"
            df_SingleFunc = df.groupby(g_l_keys).agg(*[ eval(f"{y}") for y in strAggFunc(g_l_keys, g_values) ])
            # outFileToS3(df_SingleFunc, time_flag, file_name)
            # list_df_all.append(file_name)

            # OrderLastFirst 开窗函数特殊处理
            # list_df_all.extend(dealOrderByLastFirst(df, g_l_keys, g_values, time_flag)) 

            # dict_groupby_out_s3 = {}
            # for i in list_df_all:
            #     dict_groupby_out_s3[i] = readFileS3(time_flag, i, spark)

            # df_out = eval(strJoinGroupDf(dict_groupby_out_s3, 'dict_groupby_out_s3', g_l_keys))
            return df_SingleFunc


        def pivotAndOtherColumns(df, g_identifiers, g_otherColumns, g_pivot_keyColumns, g_pivot_valueColumns, g_pivot_globalCount):
            # 如果g_otherColumns的column 在 g_identifiers 中，则g_identifiers减去该column 作为 g_groupByOn_final
            if len([i for i in g_identifiers if i in [ i['column'] for i in g_otherColumns]]) > 0:
                g_groupByOn_final = [i for i in g_identifiers if i not in [ i['column'] for i in g_otherColumns]]
            else:
                g_groupByOn_final = g_identifiers

            # pivot
            df_for_pivot = groupbyFunc(df, (g_groupByOn_final + g_pivot_keyColumns), g_pivot_valueColumns)
            df_for_pivot = df_for_pivot.withColumn('col_pivot', func.concat_ws("_", *g_pivot_keyColumns))
            l_pivot_cols = list(set(df_for_pivot.columns) -set(['col_pivot'])- set(g_groupByOn_final + g_pivot_keyColumns))
            df_out_pivot = df_for_pivot.groupby(g_groupByOn_final).pivot("col_pivot").agg(*[func.first(i).alias(i) for i in l_pivot_cols])

            # otherColumns 合并 df_out_pivot
            if len(g_otherColumns) > 0:
                df_otherColumns = groupbyFunc(df, g_groupByOn_final, g_otherColumns)
                df_out = df_out_pivot.join(df_otherColumns, on=g_groupByOn_final, how='left')
            else:
                df_out = df_out_pivot

            df_out = df_out.orderBy(g_groupByOn_final)

            return df_out


        def execute(**kwargs):
            spark = kwargs['spark']
            df = kwargs['df_$g_input$']

            project_id = "$project_id$"
            job_id = "$job_id$"

            dag_conf = get_ds_with_index(project_id, job_id)

            g_preFilter = $g_preFilter$
            g_computedColumns = $g_computedColumns$
            g_identifiers = $g_identifiers$
            g_pivot = $g_pivot$
            g_otherColumns = $g_otherColumns$

            dag_conf_prop = json.loads(dag_conf["prop"])
            if dag_conf_prop:
                g_preFilter = code_free(g_preFilter, dag_conf_prop, kwargs.get("codeFree", {}))
                g_computedColumns = code_free(g_computedColumns, dag_conf_prop, kwargs.get("codeFree", {}))

            g_pivot_globalCount = g_pivot['globalCount']
            g_pivot_keyColumns = g_pivot['keyColumns']
            g_pivot_valueColumns = g_pivot['valueColumns']
            g_pivotedColumnType = g_pivot['pivotedColumnType']

            # ===============  执行  =================
            time_flag = str(time.time()).replace(".", "_")
            
            # preFilter
            if g_preFilter['enabled'] == True:
                df = df.where(g_preFilter['expr'])
                if g_preFilter['distinct'] == True:
                    df = df.distinct()

            # computedColumns
            if len(g_computedColumns) > 0:
                for i_args in g_computedColumns:
                    df = df.withColumn(i_args['name'],  func.expr( i_args['expr'] ) ) \
                            .withColumn(i_args['name'], col(i_args['name']).cast( i_args['type'] ) )

            # globalCount对 keyColumns 进行统计
            if g_pivot['globalCount'] == True:
                df = df.withColumn('temp_groupbykey', func.lit('temp_groupbykey') )
                g_pivot_valueColumns.append({'avg': False, 'column': 'temp_groupbykey', 'concat': False, 'concatDistinct': False, 'concatSeparator': ',', 'count': True, 'countDistinct': False, 'first': False, 'firstLastNotNull': False, 'last': False, 'max': False, 'min': False, 'orderColumn': '11111', 'stddev': False, 'sum': False})
            # pivots 执行
            if g_pivotedColumnType == 'all':
                df = pivotAndOtherColumns(df, g_identifiers, g_otherColumns, g_pivot_keyColumns, g_pivot_valueColumns, g_pivot_globalCount)
                df = df.toDF(*[c.replace('temp_groupbykey_', '') for c in df.columns])

            return {"out_df":df}
