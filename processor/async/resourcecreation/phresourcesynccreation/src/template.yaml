template:
  phjob.py:
    args:
      - key: projectId
        Index: 0
      - key: dagName
        Index: 1
      - key: out_version
        Index: 2
      - key: version_col
        Index: 3
      - key: lack_path
        Index: 4
      - key: output
        Index: 5
    content: |-
        from pyspark.sql.functions import col

        def getInputPath(projectId, inputs, lack_path):
            return f"{lack_path}/{projectId}/{inputs}"

        def getOutputPath(projectId, output, lack_path):
            return f"{lack_path}/{projectId}/{output}"

        def choiceVersion(version_col, version, df):
            if len(version) > 0:
                df = df.where(col(f"{version_col}").isin(version))
            return df
    
        def execute(**kwargs):
            spark = kwargs['spark']
            df = spark.read.parquet(getInputPath($projectId$, $inputs$, $lack_path$) )
            df = choiceVersion($version_col$, $version$, df)
            df.write.format("parquet") \
                     .mode("append").partitionBy("traceId") \
                     .save(getOutputPath($projectId$, $output$, $lack_path$))
            return {"df_out":df}
            
  phmain.py:
    args:
      - key: a
        Index: 0
    content: |-
        # -*- coding: utf-8 -*-
        """alfredyang@pharbers.com.

        This is job template for Pharbers Max Job
        """
        import os
        import re
        import json
        import time
        import boto3
        import click
        import requests
        import traceback

        from phjob import execute
        from pyspark.sql import SparkSession
        from pyspark.sql.functions import lit, col, struct, to_json, json_tuple
        from pyspark.sql.types import StructType, StringType
        from phcli.ph_logs.ph_logs import phs3logger, LOG_DEBUG_LEVEL
        from functools import reduce
        from clickhouse_driver import Client
        from boto3.dynamodb.conditions import Key

        dynamodb_resource = boto3.resource("dynamodb", region_name="cn-northwest-1")

        tenant = "pharbers"

        lake_prefix = f"s3://ph-platform/2020-11-11/lake/{tenant}/"

        def get_spark_sessioin():
            config = {}

            os.environ["PYSPARK_PYTHON"] = "python3"
            spark = SparkSession.builder.master("yarn")
            default_config = {
                "spark.sql.codegen.wholeStage": False,
                "spark.sql.execution.arrow.pyspark.enabled": "true",
            }
            default_config.update(config)
            for k, v in default_config.items():
                spark = spark.config(k, v)
            spark = spark.enableHiveSupport() \
                .getOrCreate()

            access_key = os.getenv("AWS_ACCESS_KEY_ID")
            secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")
            if access_key is not None:
                spark._jsc.hadoopConfiguration().set("fs.s3a.access.key", access_key)
                spark._jsc.hadoopConfiguration().set("fs.s3a.secret.key", secret_key)
                spark._jsc.hadoopConfiguration().set("com.amazonaws.services.s3.enableV4", "true")
                spark._jsc.hadoopConfiguration().set("fs.s3a.impl","org.apache.hadoop.fs.s3a.S3AFileSystem")
                spark._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "s3.cn-northwest-1.amazonaws.com.cn")
            return spark

        spark = get_spark_sessioin()

        def transform_other_df2_spark_df(runtime, df, kwargs):
            df_choose = {
                "pyspark": lambda df: df,
                "python3": lambda df: spark.createDataFrame(df),
                "prepare": lambda df: df
            }
            return df_choose[runtime](df)

        def createClickhouseTableSql(df, table_name, database='default', order_by='', partition_by='version'):
            def getSchemeSql(df):
                file_scheme = df.dtypes
                sql_scheme = ""
                for i in file_scheme:
                    if i[0] in [order_by, partition_by]:
                        # 主键和分区键不支持null
                        coltype = i[1].capitalize()
                    else:
                        coltype = f"Nullable({i[1].capitalize()})"
                    sql_scheme += f"`{i[0]}` {coltype},"
                sql_scheme = re.sub(r",$", "", sql_scheme)
                return sql_scheme
            return f"CREATE TABLE IF NOT EXISTS {database}.`{table_name}`({getSchemeSql(df)}) ENGINE = MergeTree() ORDER BY tuple({order_by}) PARTITION BY {partition_by};"
            # return f"CREATE TABLE IF NOT EXISTS {database}.`{table_name}`({getSchemeSql(df)}) ENGINE = MergeTree()"

        def clickhouse_client(project_ip, project_name, logger):
            ch_client = Client(
                host=project_ip
            )

            return ch_client

        def get_sample_df(output_id, project_id, df):

            table_name = "dataset"
            key = {
                "id": output_id,
                "projectId": project_id
            }
            dataset_item = get_dynamodb_item(table_name, key)
            sample = dataset_item.get("sample", "F_1")
            df_sample = sample.split("_")[0]
            df_count = sample.split("_")[1]
            if df_sample == "F":
                sample_df = df.limit(int(df_count) * 10000)
            elif df_sample == "R":
                fraction = float(int(df_count) * 20000 / df.count())
                if fraction >= 1.0:
                    fraction = 1.0
                sample_df = df.sample(withReplacement=False, fraction=fraction)
                sample_df = sample_df.limit(int(df_count) * 10000)

            return sample_df

        def df_out(ph_conf, output_version, ch_client, df, project_ip, project_id,  table, output_id, mode="append", numPartitions = 2, database='default'):

            # 写入s3
            #table_name = "_".join(table.split("_")[1:])
            #path = f"{lake_prefix}{project_id}/{table_name}/"

            #consistency_df = convert_consistency_schema(df, get_df_schema2json(df))

            #consistency_df.repartition(numPartitions).write.format("parquet") \
            #            .mode("append") \
            #            .partitionBy("traceId") \
            #            .save(path)

            # 先判断 clickhouse 中是否有数据
            sql = f"SELECT COUNT(*) from `{table}`"
            res = ch_client.execute(sql)
            count = res[0][0]
            if count == 0:
                # 进行sample 策略
                sample_df = get_sample_df(output_id, project_id, df).drop("traceId")

                # 写入clickhouse
                sample_df.write.format("jdbc").mode(mode) \
                    .option("url", f"jdbc:clickhouse://{project_ip}:8123/{database}") \
                    .option("dbtable", f"`{table}`") \
                    .option("driver", "ru.yandex.clickhouse.ClickHouseDriver") \
                    .option("user", "default") \
                    .option("password", "") \
                    .option("batchsize", 1000) \
                    .option("socket_timeout", 300000) \
                    .option("numPartitions", numPartitions) \
                    .option("rewrtieBatchedStatements", True) \
                    .save()

                putOutputSchema(output_id, project_id, sample_df, output_version)

            # 创建version表
            df_versions = df.select('version').distinct()
            for i_version in df_versions:
                put_item_to_version(output_id, project_id, i_version, ph_conf.get("ownerId"))

        def get_dynamodb_item(table_name, key):
            table = dynamodb_resource.Table(table_name)
            res = table.get_item(
                Key=key,
            )
            return res.get("Item")

        def put_dynamodb_item(table_name, item):
            table = dynamodb_resource.Table(table_name)
            table.put_item(
                Item=item
            )

        def put_scheme_to_dataset(output_id, project_id, schema_list, output_version):

            table_name = "dataset"
            key = {
                "id": output_id,
                "projectId": project_id
            }
            dataset_item = get_dynamodb_item(table_name, key)
            dataset_item.update({"schema": json.dumps(schema_list, ensure_ascii=False)})
            #dataset_item.update({"version": json.dumps(output_version, ensure_ascii=False)})
            put_dynamodb_item(table_name, dataset_item)

        def put_item_to_version(output_id, project_id, output_version, owner):

            table_name = "version"
            version_item = {}
            version_item.update({"id": project_id + "_" + output_id})
            version_item.update({"projectId": project_id})
            version_item.update({"datasetId": output_id})
            version_item.update({"name": output_version})
            version_item.update({"date": str(int(round(time.time() * 1000)))})
            version_item.update({"owner": owner})

            put_dynamodb_item(table_name, version_item)

        def putOutputSchema(output_id, project_id, df, output_version):

            def getSchemaSql(df):
                schema_list = []
                file_scheme = df.dtypes
                for i in file_scheme:
                    schema_map = {}
                    schema_map["src"] = i[0]
                    schema_map["des"] = i[0]
                    schema_map["type"] = i[1].capitalize().replace('Int', 'Double')
                    schema_list.append(schema_map)
                return schema_list

            schema_list = getSchemaSql(df)
            res = put_scheme_to_dataset(output_id, project_id, schema_list, output_version)

        def get_ds_with_index(dsName, projectId):

            ds_table = dynamodb_resource.Table('dataset')
            res = ds_table.query(
                IndexName='dataset-projectId-name-index',
                KeyConditionExpression=Key("projectId").eq(projectId)
                                       & Key("name").begins_with(dsName)
            )
            return res["Items"][0]

        def createOutputs(runtime, args, ph_conf, output, project_ip, project_id, project_name, output_version, logger):

            out_df = args.get("out_df")
            # out_df = transform_other_df2_spark_df(runtime, out_df, args)

            if out_df:
                # out_df = changeIntToDouble(out_df)
                table_name = project_id + "_" + output
                output_id = get_ds_with_index(output, project_id).get("id")

                logger.debug(table_name)
                logger.debug(output_version)
                logger.debug(table_name)
                # 获取clickhouse_client
                ch_client = clickhouse_client(project_ip, project_name, logger)

                sql_create_table = createClickhouseTableSql(out_df, table_name)
                logger.debug(sql_create_table)
                logger.debug("根据df创建建表语句")

                # 因为schema可能会与原不一致导致插入错误，在此之前会进行删除表操作
                ch_client.execute(f"DROP TABLE IF EXISTS default.`{project_id}_{output}`")

                ch_client.execute(sql_create_table)
                logger.debug("根据建表语句创建表成功")

                # 获取df count
                logger.debug("输出df的count")
                add_traceId_df.show(10)
                logger.debug(add_traceId_df.count())
                df_out(ph_conf, output_version, ch_client, add_traceId_df, project_ip, project_id, table_name, output_id)
                logger.debug("向创建的表写入数据成功")


        @click.command()
        @click.option('--owner')
        @click.option('--dag_name')
        @click.option('--run_id')
        @click.option('--job_full_name')
        @click.option('--project_ip')
        @click.option('--ph_conf')
        def debug_execute(**kwargs):
            try:
                logger = phs3logger("emr_log", LOG_DEBUG_LEVEL)
                args = $args_scripts$
                output = $output$
                project_id = $projectId$
                project_name = $projectId$
                runtime = $runtime$

                project_ip = kwargs.get("project_ip")
                ph_conf = json.loads(kwargs.get("ph_conf", {}))
                user_conf = ph_conf.get("userConf", {})
                ds_conf = ph_conf.get("datasets", {})
                logger.debug("打印 user_conf")
                logger.debug(user_conf)
                logger.debug(type(user_conf))
                logger.debug("打印 ds_conf")
                logger.debug(ds_conf)
                logger.debug(type(ds_conf))
                args.update(user_conf)
                args.update({"ds_conf": ds_conf})
                args.update({"spark": spark})

                args.update(kwargs)
                output_version =  args.get("run_id") + "_" + ph_conf.get("showName")

                # df_map = create_input_df(runtime, inputs, args, project_id, project_name, output_version, logger)
                # args.update(df_map)
                result = execute(**args)

                args.update(result if isinstance(result, dict) else {})
                logger.debug("job脚本返回输出df")
                logger.debug(args)

                createOutputs(runtime, args, ph_conf, output, project_ip, project_id, project_name, output_version, logger)

                return result
            except Exception as e:
                logger = phs3logger("emr_log")
                logger.error(traceback.format_exc())
                print(traceback.format_exc())
                raise e

        if __name__ == '__main__':
            debug_execute()

